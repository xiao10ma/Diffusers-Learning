{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "\n",
    "model_id = \"/data/duantong/mazipei/HuggingFace-Download-Accelerator/data/hf_cache/hub/models--google--ddpm-cat-256/snapshots/82ca0d5db4a5ec6ff0e9be8d86852490bc18a3d9\"\n",
    "ddpm = DDPMPipeline.from_pretrained(model_id, use_safetensors=True).to(\"cuda\")\n",
    "\n",
    "image = ddpm(num_inference_steps=25).images[0]\n",
    "image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decompose DDPM pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "\n",
    "scheduler = DDPMScheduler.from_pretrained(model_id)\n",
    "model = UNet2DModel.from_pretrained(model_id, use_safetensors=True).to(\"cuda\")\n",
    "\n",
    "scheduler.set_timesteps(50)\n",
    "\n",
    "scheduler.timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sample_size = model.config.sample_size\n",
    "\n",
    "noise = torch.randn((1, 3, sample_size, sample_size)).to(\"cuda\")\n",
    "input = noise\n",
    "\n",
    "for t in scheduler.timesteps:\n",
    "    with torch.no_grad():\n",
    "        noisy_scheduler = model(input, t).sample\n",
    "    previous_noisy_sample = scheduler.step(noisy_scheduler, t, input).prev_sample\n",
    "    input = previous_noisy_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# [-1, 1]  / 2 -> [-0.5, 0.5] + 0.5 -> [0, 1]\n",
    "image = (input / 2 + 0.5).clamp(0, 1).squeeze()\n",
    "image = (image.permute(1, 2, 0) * 255).round().to(torch.uint8).cpu().numpy()\n",
    "image = Image.fromarray(image)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deconstruct the Stable Diffusion pipeline\n",
    "\n",
    "DDPM only contains a UNet model; the stable diffusion has three separate components:\n",
    "1. VAE: for compression and decompression of the image(image space -> latent space -> image space)\n",
    "2. Text encoder: for encoding the text prompt\n",
    "3. UNet: for generating the image conditioned on the text prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "\n",
    "model_id = \"\"\n",
    "vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\", use_safetensors=True)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\", use_safetensors=True)\n",
    "unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\", use_safetensors=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swap scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UniPCMultistepScheduler\n",
    "\n",
    "scheduler = UniPCMultistepScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "torch_device = \"cuda\"\n",
    "vae.to(torch_device)\n",
    "text_encoder.to(torch_device)\n",
    "unet.to(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"a photograph of an astronaut riding a horse\"]\n",
    "height = 512\n",
    "width = 512\n",
    "\n",
    "num_inference_steps = 25\n",
    "# guidance scale: how much the model should pay attention to the text prompt\n",
    "# the bigger the scale, the more attention to the text prompt and vice versa\n",
    "guidance_scale = 7.5\n",
    "generator = torch.Generator(device=torch_device).manual_seed(0)\n",
    "batch_size = len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tokenizer(\n",
    "    prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youâ€™ll also need to generate the unconditional text embeddings which are the embeddings for the padding token. These need to have the same shape (batch_size and seq_length) as the conditional text_embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
    "uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s concatenate the conditional and unconditional embeddings into a batch to avoid doing two forward passes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create random noise\n",
    "\n",
    "ðŸ’¡ The height and width are divided by 8 because the vae model has 3 down-sampling layers. You can check by running the following:\n",
    "```\n",
    "2 ** (len(vae.config.block_out_channels) - 1) == 8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = torch.randn(\n",
    "    (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    "    device=torch_device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoise the image\n",
    "Start by scaling the input with the initial noise distribution, sigma, the noise scale value, which is required for improved schedulers like UniPCMultistepScheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = latents * scheduler.init_noise_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to create the denoising loop thatâ€™ll progressively transform the pure noise in latents to an image described by your prompt. Remember, the denoising loop needs to do three things:\n",
    "\n",
    "1. Set the schedulerâ€™s timesteps to use during denoising.\n",
    "2. Iterate over the timesteps.\n",
    "3. At each timestep, scale the model input, call the UNet model to predict the noise residual and pass it to the scheduler to compute the previous noisy sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "for t in tqdm(scheduler.timesteps):\n",
    "    # expand latents because of the condition and the un-condition\n",
    "    latent_model_input = torch.cat([latents * 2])\n",
    "    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "    # predict the conditional noise\n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    # perform guidance\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # è‡³æ­¤ï¼Œæ‰ç®—å®Œå…¨ç¡®å®šäº† noise\n",
    "    # ä¸‹ä¸€æ­¥æ‰æ˜¯åŽ»å™ª\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler.step(noise_pred, t, latents).prev_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latens = 1 / 0.18215 * latents\n",
    "with torch.no_grad():\n",
    "    image = vae.decode(latents).sample\n",
    "\n",
    "image = (image / 2 + 0.5).clamp(0, 1).squeeze()\n",
    "image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()\n",
    "image = Image.fromarray(image)\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4DG2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
